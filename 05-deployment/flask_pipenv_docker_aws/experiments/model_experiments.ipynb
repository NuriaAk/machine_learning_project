{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a02cff2",
   "metadata": {},
   "source": [
    "# Saving and loading the model\n",
    "\n",
    "Before we can save a model, we have to train it. \n",
    "Below, is all the code necessary for model training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5079a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-23 10:44:21--  https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 977501 (955K) [text/plain]\n",
      "Saving to: ‘data-week-3.csv’\n",
      "\n",
      "data-week-3.csv     100%[===================>] 954.59K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2025-10-23 10:44:21 (73.9 MB/s) - ‘data-week-3.csv’ saved [977501/977501]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    " \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Data preparation\n",
    "\" When using Jupyter Notebook, it’s important to note that ‘!’ indicates the execution \"\n",
    "\"of a shell command, and the ‘$’ symbol, as seen in ‘$data,’ is the way to reference \"\n",
    "\"data within this shell command.\"\n",
    "\n",
    "data = 'https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-03-churn-prediction/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "!wget $data -O data-week-3.csv\n",
    "df = pd.read_csv('data-week-3.csv')\n",
    " \n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    " \n",
    "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
    " \n",
    "for c in categorical_columns:\n",
    "    df[c] = df[c].str.lower().str.replace(' ', '_')\n",
    " \n",
    "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
    "df.totalcharges = df.totalcharges.fillna(0)\n",
    " \n",
    "df.churn = (df.churn == 'yes').astype(int)\n",
    "\n",
    "# Data splitting\n",
    " \n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']\n",
    " \n",
    "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "       'phoneservice', 'multiplelines', 'internetservice',\n",
    "       'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
    "       'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
    "       'paymentmethod']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a743fb6",
   "metadata": {},
   "source": [
    "To the train function we have the training dataframe and the target values y_train, and the third argument is C which is a LogisticRegression parameter for our model. \n",
    "\n",
    "At first we use DictVectorizer to encode the categorical columns, remember the numerical columns are ignored here. Then we use a logistic regression model for training (fit function) based on the training data (X_train and y_train). To apply the model later we need to return the DictVectorizer and the model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab54d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train, C=1.0):\n",
    "    dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    " \n",
    "    model = LogisticRegression(C=C, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    " \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd3a6e",
   "metadata": {},
   "source": [
    "For the predict function we also need a dataframe where we can provide a prediction for. We need to get the dictionaries to get the X to make a prediction on. We return the predicted probability for churning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ff370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, dv, model):\n",
    "     dicts = df[categorical + numerical].to_dict(orient='records')\n",
    " \n",
    "     X = dv.transform(dicts)\n",
    "     y_pred = model.predict_proba(X)[:,1]\n",
    " \n",
    "     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "838daa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C is the value for the Logistic Regression model\n",
    "# ‘n_splits’ parameter tells us how many splits we’re going to use in K-Fold cross-validation\n",
    "\n",
    "C = 1.0\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61162fc",
   "metadata": {},
   "source": [
    "Next we implement K-Fold cross validation onusing train and validation datasets, the for loop loops over all folds and does a training for each fold. After that we calculate the roc_auc_score and collect the values. At the end the mean score and the standard deviation for all folds are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328e4edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:470: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1.0 0.842 +- 0.007\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)  \n",
    " \n",
    "scores = []\n",
    " \n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    " \n",
    "    y_train = df_train.churn.values\n",
    "    y_val = df_val.churn.values\n",
    " \n",
    "    dv, model = train(df_train, y_train, C=C)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    " \n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    " \n",
    "print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df1106cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8446829053857807,\n",
       " 0.8451798602834062,\n",
       " 0.8332289785269917,\n",
       " 0.8347808882778027,\n",
       " 0.8517225691067114]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc52a2",
   "metadata": {},
   "source": [
    "Last step is to train the final model based on the full_train data and make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cac55391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8583517501381259"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv, model = train(df_full_train, df_full_train.churn.values, C=1.0)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "y_test = df_test.churn.values\n",
    " \n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc618b86",
   "metadata": {},
   "source": [
    "So far, our model only exists in the Jupyter notebook. To use it in a web service for customer scoring, we first need to save the model so it can be loaded later. To save the model, we’ll use pickle, a built-in Python library for serializing objects. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fdffced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa44bc",
   "metadata": {},
   "source": [
    "First, let’s name our model file. Here are two ways to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23f75bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'model_C=%s.bin' % C\n",
    "# or\n",
    "output_file = f'model_C={C}.bin'\n",
    "# Output: 'model_C=1.0.bin'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ef123",
   "metadata": {},
   "source": [
    "Next, we create and write to the file. The 'wb' mode means write in binary. We save both the DictVectorizer and the model, since the model alone can’t convert customer data into feature matrices. Don’t forget to close the file to ensure it’s properly saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8df840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = open(output_file, 'wb')\n",
    "pickle.dump((dv, model), f_out)\n",
    "f_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ecdd8f",
   "metadata": {},
   "source": [
    "A safer way is to use a with statement, which automatically closes the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05851d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'wb') as f_out:\n",
    "    pickle.dump((dv, model), f_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e5c02",
   "metadata": {},
   "source": [
    "# Loading the model with Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d08a7",
   "metadata": {},
   "source": [
    "For loading the model we’ll also use pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "531afbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9919a193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DictVectorizer(sparse=False), LogisticRegression(max_iter=1000))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = 'model_C=1.0.bin'\n",
    "\n",
    "# Use  ‘with’ statement for loading the model. Here, ‘rb’ denotes Read Binary. \n",
    "# The ‘load’ function from pickle returns both the DictVectorizer and the model.\n",
    "\n",
    "with open(model_file, 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)\n",
    " \n",
    "dv, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96246676",
   "metadata": {},
   "source": [
    "After loading the model, let’s use it to score one sample customer. Before we can apply the predict function we need to turn it into a feature matrix. The DictVectorizer expects a list of dictionaries, that’s why we create a list with one customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3f9433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = {\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'yes',\n",
    "    'dependents': 'no',\n",
    "    'phoneservice': 'no',\n",
    "    'multiplelines': 'no_phone_service',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'no',\n",
    "    'onlinebackup': 'yes',\n",
    "    'deviceprotection': 'no',\n",
    "    'techsupport': 'no',\n",
    "    'streamingtv': 'no',\n",
    "    'streamingmovies': 'no',\n",
    "    'contract': 'month-to-month',\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'electronic_check',\n",
    "    'tenure': 1,\n",
    "    'monthlycharges': 29.85,\n",
    "    'totalcharges': 29.85\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f4c2aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
       "         0.  ,  1.  ,  0.  ,  0.  , 29.85,  0.  ,  1.  ,  0.  ,  0.  ,\n",
       "         0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,\n",
       "         0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,\n",
       "         0.  ,  1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  , 29.85]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dv.transform([customer])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51efe674",
   "metadata": {},
   "source": [
    "We use predict function to get the probability that this particular customer is going to churn which is the second element in the output, so we need to set the row=0 and column=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6f8aa4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6273177838726128)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)\n",
    "# Output: array([[0.36364158, 0.63635842]])\n",
    " \n",
    "model.predict_proba(X)[0,1]\n",
    "# Output: 0.6363584152758612"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba61e5",
   "metadata": {},
   "source": [
    "We can turn the Jupyter Notebook code into a Python file. One easy way of doing this is click on “File” -> “Download as” and then “Python (.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
